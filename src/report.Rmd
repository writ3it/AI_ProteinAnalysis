---
title: "Basic analysis of ligands"
author: "writ3it"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:  
    highlight: kate
    number_sections: yes
    theme: united
    toc: true
    toc_float: true
params:
  source_url: "https://zenodo.org/record/1040778/files/all_summary.7z"
  csv_name: "big_summary"
  zipped: true
  isSample: true
  cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=!params$cache)
working_dir <-getwd()
```

# Summary

## Dataset
Master dataset contains 591043 records. First record is a header. Each next row contains description of ligand computed from PDB database. Contains ligands queried and described in the Kowiel et al. paper "Automatic recognition of ligands in electron density by machine learning methods". 

Download [all_summary.7z](https://zenodo.org/record/1040778/files/all_summary.7z")

```{r dataset_size}
#number of rows in sample mode
sampleRowsNo <- 4000
# number of rows in main dataset (with excess)
targetRowsNo <- 600000
```

*Report presents results of computing part of dataset. Computed dataset contain first n rows (see below at Size of dataset) of master dataset.*

### Smaller dataset

If you would like to test code on smaller piece you can create it quickly with code:
```{bash, eval=FALSE}
#replace 100K with any number of rows
head -n 100000 data/all_summary.csv > data/big_summary.csv
```

This report is created with parameterized Rmarkdown doc. When isSample=true for rendering will be used first 2K rows.

## Analysis

Description of ligand contains few diffrent datatypes. These are mainly numerics like descriptive statistics of geometry. Because each ligand has diffrend shape some statistics don't appear. Some attributes contains data packed at JSON format. res_name attribute contains target class which is ligand name. Description of geometry need some knowladge about crystallography and PDB data formats. My knowladge is little therefore *analysis isn't big succes*, but example of work with R machine learning and analysis tools. 

### Correlation method
While analysis will be use spearman correlation because distribution of values (see charts below) and a large number of attributes suggest me that attributes doesn't linear correlated. Therefore spearman method will be a better choice.
```{r cor_method}
cor_method<-"spearman"
```

### Regression

Regression analysis in this report is an good admission to proper classification. The following parameters have been selected experimentally. Because dataset is big, training set will be 90% of whole. Choosed algorithm is Linear Regression which is standard algorithm for numeric prediction.
```{r reg_params}
part.reg<-0.9
```

### Classification
The following parameters have been selected experimentally. Cutoff is strong depended on number of attributes. (Theoretically, for data form physical experiments, the higher is better). Dataset will be partitioned like 80% - training set 20% testing set with repeated k-folds cross validation. Method of training controll is dictated by distribution of records in classes.

I suppose that number of atoms or electrons will improve classification therefore dataset will be splitted. On first part will be trained atom number prediction model, on second part data will be completed with predicition. Next, second part will be used as master data for classification (next splits etc.).
```{r cla_params}
split<-0.2 # atoms prediction parts
cutoff.cla<-0.79
part.cla<-0.8
ntree<-50
```

## Rendering

Report rendering command:
```{bash, eval=FALSE}
export RSTUDIO_PANDOC=/usr/lib/rstudio/bin/pandoc
Rscript src/build.r
```




# Data preparation

## Used R libraries 

- psych - nice, describe function
- ggplot2 - usefull api to create plots
- plotly - ggplot2 is good but plotly plots are more interactive and flexy
- dplyr - human-friendly data processing api
- friendlyeval - dynamic (by string) column selection while dlypr processing
- reshape2 - very usefull melt function
- DT - "DataTable" widget which makes tables powerfull
- archive - for 7z extraction
- grid,gridExtra - for plot grid
- caret - ML library, need dependencies = 'obliqueRF','logicFS'


```{r libs, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
if (!require("psych")){ install.packages('psych') }
library(psych)

if (!require("ggplot2")){ install.packages('ggplot2') }
library(ggplot2)

if (!require("dplyr")){ install.packages('dplyr') }
library(dplyr)

if (!require("DT")){ install.packages('DT') }
library(DT)

if (!require("DT")){ devtools::install_github("milesmcbain/friendlyeval") }
library(friendlyeval)

library(reshape2)

if (!require("archive")){  devtools::install_github("jimhester/archive")}
library(archive)

library(plotly)
library(grid)

if (!require("gridExtra")){ install.packages("gridExtra") }
library(gridExtra)
if (!require("caret")){ install.packages("caret",dependencies=c("logicFS")) }
library(caret)
```

## Initialization code
For the repeatability of the analysis
```{r init}
set.seed(23) 
```

```{r dtable, echo=FALSE}
prettyTable <- function(table_df, round_columns=numeric(), round_digits=2) {
    DT::datatable(table_df, style="bootstrap", filter = "top", rownames = FALSE, extensions = "Buttons", options = list(dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print'))) %>%
    formatRound(round_columns, round_digits)
} # same look of tables
```

## Data loading

### Downloading dataset
First step is download dataset. It's large so will be "cached" to use in fututre.
```{r loading}
tempFilePath <- paste(working_dir,"/../data/temp.7z",sep='')
dataFilePath <- paste(working_dir,"/../data/",params$csv_name,".csv",sep='')
if (!file.exists(dataFilePath)){
  message("Downloading data from: ",params$source_url);
  download.file(params$source_url,tempFilePath)
  archive_extract(tempFilePath,"./data");
  if (!file.exists(dataFilePath)){
    stop("Data file not found");
  }
} else {
  message("Data was downloaded previously. Be careful!")
}
```



### Reading sample data
Next, load data to memory. In next code examples data will be sotred at **data** variable.
```{r sample_data}

# heuristic of attributes datatypes, this increase performance of main data loading
sample <- read.table(dataFilePath, nrows=sampleRowsNo, sep = ";", header = TRUE, na.strings = "nan",  quote = "'",  )
classes <- sapply(sample,class)
# need some correction in datatypes list 
classes <- ifelse(classes == "integer","numeric",classes)
classes["res_id"]<-"character"
# if should be, load all data
if (!params$isSample){
  print("Loading full dataset")
  sample <-read.table(dataFilePath, header = TRUE,  nrows=targetRowsNo, sep = ";", quote = "'", colClasses=classes, na.strings = "nan",                     comment.char = "")
} else {
  print("Loading partial dataset")
}
# will be usefull
attrib.classes <- classes
```
```{r init_data,echo=FALSE}
data <- sample
rm(sample)
```

### Dataset attributes lists 
For next parts necessary is to define some information about data, attributes and paramteres.
```{r attribs}
# number of most frequently ligands which will be analized 
noTopClasses <- 50

# target class for classification
attrib.target_class <- "res_name"

# target attributes for regression
attrib.reg.targets <- c("dict_atom_non_h_count","dict_atom_non_h_electron_sum")

#all attributes
attrib.all <- colnames(data)

#"local" attributes
attrib.local <- c( attrib.all[ grepl("local_res_", attrib.all)],'local_cut_by_mainchain_volume','local_near_cut_count_C','local_near_cut_count_other','local_near_cut_count_S','local_near_cut_count_O','local_near_cut_count_N')

# dict_atom attribs
attrib.dict <- attrib.all[grepl("dict_atom_", attrib.all)]

# parts 
attrib.part <- attrib.all[grepl("part_",attrib.all)]

# parts_01
attrib.part_01 <- attrib.all[grepl("part_01_",attrib.all)]

# skeleton
attrib.skeleton <- attrib.all[grepl("skeleton_",attrib.all)]

# resolution
attrib.res <- "resolution"

# params
attrib.params <- c("fo_col","fc_col","weight_col","grid_space","solvent_radius","solvent_opening_radius")

# uknown columns
attrib.unknown <- c("blob_coverage","blob_volume_coverage_second","resolution_max_limit","FoFc_square_std","res_coverage","res_volume_coverage","FoFc_mean","FoFc_min","blob_volume_coverage","res_volume_coverage_second","FoFc_std","FoFc_max","resolution")

# illegal attribs for classifications
attrib.illegal <- c(c(
  "title",
  "pdb_code",
  "res_name",
  "res_id",
  "chain_id",
  "weight_col", #is na!
  "skeleton_data"
),
attrib.unknown,
attrib.params
)

# attribs for experiments
attrib.exp_01 <- c("local_res_atom_non_h_count","local_res_atom_non_h_electron_sum")
attrib.exp_02 <- c("local_res_atom_non_h_count","dict_atom_non_h_count","dict_atom_non_h_electron_sum","local_res_atom_non_h_electron_sum")

# illegal attributes but needed 
attrib.trusted <- c(attrib.exp_01  , attrib.part_01,attrib.exp_02)

# legal attributes 
attrib.legal<-c(attrib.trusted,setdiff(attrib.all, c(attrib.illegal,attrib.local, attrib.dict)))

# legal numeric attributes
.types <- attrib.classes[attrib.legal]
attrib.legal.numeric <- names(.types)[.types=="numeric"]

# shape attributes
attrib.part.shape <- attrib.legal[grepl("_shape_",attrib.legal)]

# density attributes
attrib.part.density <- attrib.legal[grepl("_density_",attrib.legal)]

# excluded ligands
excluded_names <- c("UNK", "UNX", "UNL", "DUM", "N", "BLOB", "ALA", "ARG", "ASN", "ASP", "CYS", "GLN", "GLU", "GLY", "HIS", "ILE", "LEU", "LYS", "MET", "MSE", "PHE", "PRO", "SEC", "SER", "THR", "TRP", "TYR", "VAL", "DA", "DG", "DT", "DC", "DU", "A", "G", "T", "C", "U", "HOH", "H20", "WAT");
```
## Data cleansing

### Remove unnecesary ligands (project requirement)
```{r cleansing_1}
data <- data %>%
        filter(!res_name %in% excluded_names) %>%
        filter(!is.na(res_name))
```

### Find empty numeric attributes
Some empty data should be presented in part_01 charts, so it will be removed in future.
```{r removeEmptyColumns}
# list empty attributes
stats <- data %>% select(attrib.legal.numeric) 
max_min <- data_frame(max = apply(stats, 2, max),
                      min = apply(stats, 2, min),
                      columns = names(stats))
attrib.empty <- max_min$columns[max_min$min==max_min$max]
```

### Filling data na with means
```{r cleansing_2, message=FALSE}
#calculate means
replacements <<- data %>% 
  select(c(attrib.legal,attrib.target_class)) %>% 
  na.omit %>%
  group_by(!!treat_string_as_col(attrib.target_class)) %>% 
  summarize_all(funs(mean))

ids <- data[,attrib.target_class]

# fill NA with means
for (col in colnames(replacements)){
  column<-data[,col]
  badidx <- is.na(column)
  if (sum(badidx)==0){
    next
  }
  
  bad_ids <- data.frame(ids[badidx])
  colnames(bad_ids)<-c(attrib.target_class)
  vals <- data.frame(bad_ids) %>% left_join(replacements, by = attrib.target_class) %>% select(!!treat_string_as_col(col))
  new_vals <- unlist(vals, use.names = FALSE)
  data[badidx,col] <- new_vals
}

# replace other na
data <- data %>% replace(is.na(.),0)
```

### Limit data with legal attribs

```{r legal}
data <- data %>% select_(.dots=c(attrib.target_class, attrib.legal))
```

## Dataset description

### Size of dataset
```{r desc, show, echo=FALSE}
print(paste("Number of rows: ",nrow(data)))
print(paste("Number of attributes: ",ncol(data)))
print(paste("Number of legal attributes: ",length(intersect(colnames(data),attrib.legal))))
print(paste("Number of legal numeric attributes: ",length(intersect(attrib.legal,attrib.legal.numeric))))
```
### Attributes
```{r desc_attribs, echo=FALSE}
describe(data)
```




# Analysis

### Choose top 50 ligands, number of examples per class

```{r find_top_50}
top50 <- data %>% 
          group_by(!!treat_string_as_col(attrib.target_class)) %>% 
          summarize(numberOfExamples = n()) %>% 
          arrange(desc(numberOfExamples)) %>% 
          head(noTopClasses) 

prettyTable(top50)

.topClasses<-unlist(top50[,attrib.target_class],use.names = FALSE)
data <- data %>% filter( !!treat_string_as_col(attrib.target_class) %in% .topClasses )
```

### Split data to trainData and labels

```{r split}

data.train <- data %>% select_(.dots=setdiff(colnames(data),attrib.target_class))
.labels <- data %>% select_(.dots=c(attrib.target_class)) %>% droplevels()
data.labels <- .labels[,attrib.target_class]
```

### Remove zero variantion

```{r zero_var}
zv <- apply(data.train, 2, function(x) length(unique(x)) == 1)
data.train <- data.train[, !zv]
```

### Correlation
Calculate correlation
```{r correlation}
correlation <- cor(data.train,method=cor_method)
correlation.melted <- melt(correlation)
```
Correlation on heat map looks like chart below:
```{r presentation,echo=FALSE}
df.cor.mel <- data.frame(correlation.melted)  %>% mutate(value = abs(value)) %>% arrange(desc(value))

p<-ggplot(
      df.cor.mel
      ,aes(x=Var1,y=Var2, fill=value)) +
      geom_tile() +
      scale_fill_gradient(low = "white", high = "brown") +
      xlab("parameters") +
      ylab("parameters") + 
      theme(axis.text.x = element_text(angle = -90, hjust = 1))

ggplotly(p,height=600, width=700)
```

### Distribution of atoms and electrons
```{r distr, echo=FALSE,message=FALSE}
dist <- data %>% select_(.dots=c(attrib.target_class, "local_res_atom_non_h_count","local_res_atom_non_h_electron_sum"))
mdist <- melt(dist)

format_plot <- function(p,color, title){
   p+geom_histogram(alpha=0.5, position="identity", aes(y = ..density..), color="black", fill=color) + 
    scale_y_sqrt() +
    ggtitle(title) + 
    xlab("value") + 
    ylab("sqrt(cardinality)") 
}


p<- ggplot(dist,aes(x=dist$local_res_atom_non_h_count))
p1 <- format_plot(p,"yellow","Distribution of no_atoms")
p<- ggplot(dist,aes(x=dist$local_res_atom_non_h_electron_sum))
p2 <- format_plot(p,"blue","Distribution of no_electrons")
grid.arrange(p1,p2)
```

### Analyze part_01 parameters
To explore information hidden in structure of ligand preview distribution of values.

```{r part_01, echo=FALSE,message=FALSE}
parts.data <- data %>% select(c(attrib.target_class,attrib.part_01))
parts.melted <- melt(parts.data)

unames <- unlist(unique(parts.melted[,attrib.target_class]))
uvars <- unlist(unique(parts.melted[,'variable']))

button.names <- lapply(unames, function(name){
  list(
    method="restyle",
    args=list("transforms[0].value",name),
    label= name
  )
})

button.vars <- lapply(uvars, function(v){
  list(
    method="restyle",
    args=list("transforms[1].value",v),
    label= v
  )
})

       
parts.melted %>% plot_ly(
                  x=~value,
                  type="histogram",
                  transforms=list(
                    list(
                      type="filter",
                      target= ~res_name,
                      orientation = '=',
                      value = unames[1]
                    ),
                     list(
                      type="filter",
                      target= ~variable,
                      orientation = '=',
                      value = uvars[1]
                    )
                  )) %>%
 
  layout(
    title="Distribution of Values",
    updatemenus=list(
      list(
        type='dropdown',
        y=0.8,
        x=-0.1,
        buttons = button.names
      ),
      list(
        type='dropdown',
        y=0.6,
        x=-0.1,
        buttons = button.vars
      )
    ),
   yaxis = list(
      title = "Cardinality"
    ),
    annotations=list(
      list(
        x=-0.27,
        y=0.85,
        xref='paper',
        yref='paper',
        text="res_name",
        showarrow=FALSE
      ),
      list(
        x=-0.27,
        y=0.65,
        xref='paper',
        yref='paper',
        text="parameter",
        showarrow=FALSE
      )
    )
  )
```
### The 10 most incompatible classes
I understand by incmpatible sum(abs(local*,dict*))
```{r incomp, echo=FALSE}
f <- data %>% 
  select(!!treat_string_as_col(attrib.target_class),
         local_res_atom_non_h_count,
         dict_atom_non_h_count,
         dict_atom_non_h_electron_sum,
         local_res_atom_non_h_electron_sum) %>%
  arrange(res_name)

f<-f %>%
  group_by(!!treat_string_as_col(attrib.target_class)) %>%
  summarize( 
    atoms = sum(abs(local_res_atom_non_h_count-dict_atom_non_h_count)),
    electrons = sum(abs(local_res_atom_non_h_electron_sum-dict_atom_non_h_electron_sum))
    )

df.atoms <- f%>% select(!!treat_string_as_col(attrib.target_class),atoms) %>% arrange(desc(atoms)) %>% head(10)
df.electrons <- f%>% select(!!treat_string_as_col(attrib.target_class),electrons) %>% arrange(desc(electrons)) %>% head(10)
prettyTable(df.atoms)
prettyTable(df.electrons)
```


# Machine Learning
```{r free_mem}
rm(data)
```

### Remove illegal attributes before regression
```{r remove_reg1}
data.train <- data.train %>% select_(.dots=unique(c(setdiff(colnames(data.train),c( attrib.exp_01 )),attrib.exp_02)))
```

### Remove strong correlated attributes
```{r remove_strongs}
strongCorrelated <- findCorrelation(correlation,cutoff=cutoff.cla)
cn <- colnames(correlation)
featuresNames <- cn[strongCorrelated]
data.train <- data.train %>% select_(.dots=c(setdiff(colnames(data.train),featuresNames),attrib.reg.targets))
```

### Remove zero var attributes
```{r classification_select_features}
zeroVarNames<-nearZeroVar(data.train,names=TRUE,foreach=TRUE,allowParallel=TRUE)
data.train <- data.train %>% select_(.dots=c(setdiff(colnames(data.train),zeroVarNames),attrib.reg.targets))
```

### Remove top incompatibile classes
```{r top_inc}
toRemove <- unique(append(as.character(df.atoms$res_name),as.character(df.electrons$res_name)))
idx<-data.labels %in% toRemove
data.train <- data.train[-idx,]
data.labels<- data.labels[-idx]
```

## Split data
```{r splitting}
splitting <- createDataPartition(y = data.labels,p=split, list=FALSE )
```

## Predict number of electrons or atoms with regression

```{r regresion, echo=FALSE,messages=FALSE,warning=FALSE}
partition <- part.reg

ctrl <- trainControl(
  method = "repeatedcv",
  number = 2,
  repeats = 5
)
avcols <-   colnames(data.train)

for (target in attrib.reg.targets){
  illegal <- setdiff(attrib.reg.targets,target)
    features <- correlation.melted %>% 
    filter( Var1 == target & Var2 != target) %>% 
    filter( Var2 %in% setdiff(avcols,illegal)) %>%
    mutate(value = abs(value)) %>% arrange(desc(value)) %>% 
    head(20)
  cols<-c(as.character(features$Var2),target)
  
  .data <- data.train  %>% select(cols)
  #use only first part of data!
  .data <- .data[splitting,]
  inTraining <- createDataPartition(y = .data[,target],p=partition, list=FALSE )
  training <- .data[inTraining,]
  testing <- .data[-inTraining,]
  if (target == "dict_atom_non_h_count"){
  fit_atoms <- train( dict_atom_non_h_count  ~ . ,
               data=training ,
               method="lm",
               metric="RMSE",
               trControl = ctrl,
                tuneGrid = data.frame(intercept = seq(0,2,by=0.5))
                )
    
    rfClasses <- predict(fit_atoms, newdata = testing)
    print(ggplot(fit_atoms) + ggtitle(target)+ theme_bw())
    rmse<-RMSE(pred = rfClasses,obs = testing$dict_atom_non_h_count)
    r2<-R2(pred=rfClasses, obs=testing$dict_atom_non_h_count)
    print(paste(target," RMSE= ",rmse))
    print(paste(target," RSquared= ",r2))
  } else {
      fit_electrons <- train( dict_atom_non_h_electron_sum  ~ . ,
                 data=training ,
                 method="lm",
                 metric="RMSE",
                 trControl = ctrl,
                 tuneGrid = data.frame(intercept = seq(0,2,by=0.5))
                 )
    print(ggplot(fit_electrons) + ggtitle(target)+ theme_bw())
    rfClasses <- predict(fit_electrons, newdata = testing)
    rmse<-RMSE(pred = rfClasses,obs = testing$dict_atom_non_h_electron_sum)
    r2<-R2(pred=rfClasses, obs=testing$dict_atom_non_h_electron_sum)
    print(paste(target," RMSE= ",rmse))
    print(paste(target," RSquared= ",r2))
  }
}
```

## Predict ligand name

### Remove illegal attributes after regression
```{r remove_reg}
data.train <- data.train %>% select_(.dots=setdiff(colnames(data.train),c( attrib.exp_02,attrib.local, attrib.dict )))
```

### Predict number of atoms
```{r trick2} 
data.train <- data.train[-splitting,]
data.labels <- data.labels[-splitting]
estimated_atoms <-  predict(fit_atoms, data.train)
estimated_electrons <-  predict(fit_electrons, newdata = data.train)
data.train$estimated_atoms <- estimated_atoms
data.train$estimated_electrons <- estimated_electrons
```

### Training attributes, ordering
```{r tra_attr, echo=FALSE}
#sort attributes with sds for preview
colSds = sqrt(diag(cov(data.train)))
colorder<-colSds[order(unlist(colSds), decreasing = TRUE)]
data.train <- data.train %>% select_(.dots=names(colorder))
#preview
colnames(data.train)
```

### Training
```{r classification}
  partition<-part.cla
  inTraining <- createDataPartition(y = data.labels,p=partition, list=FALSE )
  
  training <- data.train[inTraining,]
  ytraining <- data.labels[inTraining]
  testing <- data.train[-inTraining,]
  ytesting <- data.labels[-inTraining]
  
  folds<-ncol(training)
  cvIndex <- createFolds(factor(ytraining), folds, returnTrain = TRUE)
  ctrl <- trainControl(index = cvIndex,
               method = 'repeatedcv', 
               number = folds,
               allowParallel=TRUE,
               returnData = FALSE,
               repeats = 2,
               verboseIter=FALSE
               )
  grid <- expand.grid(
    mtry = seq(1,ceiling(sqrt(folds)),by=1)
  )
  
  fit <- train( x = training,
                y = ytraining,
                method="parRF",
                preProcess = c("scale", "center"),
                trControl = ctrl,
                metric="Accuracy",
                maximize = TRUE,
                tuneGrid=grid,
                importance=TRUE,
                ntree=ntree
                )

  print(fit)
```
### Predict
```{r predict}
  rfClasses <- predict(fit, testing)
  confusionMatrix(data = rfClasses, ytesting)
```

### Accuracy
```{r acc}
  print(ggplot(fit) + ggtitle("Accurency ligands training")+ theme_bw())
```






